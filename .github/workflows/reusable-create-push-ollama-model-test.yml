name: create-ollama-model-reusable-test

on:
  workflow_call:
    inputs:
      debug:
        type: boolean
        required: true
        default: false
      granite_family:
        type: string
        required: true
      enable_ollama_push:
        type: boolean
        required: false
        default: false
      enable_language_jobs:
        type: boolean
        required: false
        default: false
      enable_vision_jobs:
        type: boolean
        required: false
        default: false
      enable_guardian_jobs:
        type: boolean
        required: false
        default: false
      enable_embedding_jobs:
        type: boolean
        required: false
        default: false
      target_repo_owner:
        type: string
        required: true
      target_repo_name_ext:
        type: string
        required: true
      target_ollama_org:
        type: string
        required: true
      repo_id:
        type: string
        required: true
      quantization:
        type: string
        required: true
      hf_collection_config:
        type: string
        required: true

jobs:
  ollama-continuous-delivery:
    runs-on: macos-latest
    if: ${{ inputs.enable_ollama_push == true && ( inputs.enable_language_jobs == true || inputs.enable_vision_jobs == true || inputs.enable_guardian_jobs == true || inputs.enable_embedding_jobs == true ) }}
    env:
      DEFAULT_MODEL_LICENSE: Apache-2.0
      MODEL_DOWNLOAD_DIR: models
      EXT_GGUF: .gguf
      F16_MMPROJ_MODEL_FILENAME: mmproj-model-f16.gguf
      OLLAMA_RESOURCE_DIR: resources/ollama
      OLLAMA_MODELFILE: Modelfile
      OLLAMA_LICENSE_FILE: resources/LICENSE-2.0.txt
      OLLAMA_TEMPLATE_FILE: template.txt
      OLLAMA_PARAMS_FILE: params.json
      OLLAMA_SYSTEM_FILE: system.txt
      OLLAMA_DEBUG: 0 # TODO: allow setting at workflow level...
      OLLAMA_PORT: 11434
      OLLAMA_HOST: http://127.0.0.1:11434
      OLLAMA_MODEL_PRIVATE: true
      IS_DEFAULT_QUANT: false
      HAS_PROJECTOR_MODEL: false
      HF_XET_NUM_CONCURRENT_RANGE_GETS: 4
      # NOTE: hf_xet and hf_transfer are mutually exclusive:
      # hf_xet is for Xet-enabled repos, while hf_transfer handles LFS files in regular repos.
      HF_HUB_DISABLE_XET: 0
      # HF_HUB_ENABLE_HF_TRANSFER: 0
      HF_HUB_ENABLE_HF_TRANSFER: 0
    steps:
      - uses: actions/checkout@v4
        with:
          sparse-checkout: |
            scripts/
            bin/ollama
            resources/json/latest
            resources/ollama
            resources/LICENSE-2.0.txt

      - name: confirm-environment
        run: |
          uname -a
          echo "runner.os: ${{ runner.os }}"
          echo "runner.arch: ${{ runner.arch }}"
          echo "github.workspace: ${{ github.workspace }}"
          echo "env.EXT_GGUF: ${{ env.EXT_GGUF }}"
          echo "env.EXT_NAME_F16: ${{ env.EXT_NAME_F16 }}"
          echo "env.F16_OPT_ENABLED: ${{ env.F16_OPT_ENABLED }}"
          echo "env.FIND_ARGS: ${{ env.FIND_ARGS }}"
          echo "env.MODEL_DOWNLOAD_DIR: ${{ env.MODEL_DOWNLOAD_DIR }}"

      - name: verify-secrets-present-2
        run: |
          echo "hf_token: ${{ secrets.hf_token }}"
          echo "HF_TOKEN: ${{ secrets.HF_TOKEN }}"
          echo "HF_TOKEN_IBM_GRANITE: ${{ secrets.HF_TOKEN_IBM_GRANITE }}"
          echo "OLLAMA_TOKEN: ${{ secrets.OLLAMA_TOKEN }}"

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: install-dependencies
        run: |
          python -m pip install -r ./requirements.txt
          pip list

      # Use this step to set values to the github context (shared across jobs/steps)
      # Note: using $GITHUB_OUTPUT sets values under the current step's namespace
      # whereas using $GITHUB_ENV sets values in the job's underlying environment.
      # Note: for each 'repo_id' we parse out e.g., REPO_ORG=ibm-granite REPO_NAME=granite-3.0-2b-instruct
      - name: set-github-env
        id: set_github_env
        run: |
          echo "REPO_ORG=$(dirname '${{ inputs.repo_id }}')" >> $GITHUB_ENV
          echo "REPO_NAME=$(basename '${{ inputs.repo_id }}')" >> $GITHUB_ENV

      - name: set-derivative-env-vars-1
        run: |
          echo "TARGET_HF_REPO_ID=${{inputs.target_repo_owner}}/${{env.REPO_NAME}}${{inputs.target_repo_name_ext}}" >> $GITHUB_ENV
          echo "TARGET_OLLAMA_ORG=${{ inputs.target_ollama_org }}" >> $GITHUB_ENV
          echo "BASE_NAME_QUANTIZED=${{ env.REPO_NAME }}-${{inputs.quantization}}" >> $GITHUB_ENV

      - name: set-derivative-env-vars-2
        run: |
          echo "BASE_FNAME_QUANTIZED_GGUF=${{ env.BASE_NAME_QUANTIZED}}${{env.EXT_GGUF}}" >> $GITHUB_ENV
          echo "LOCAL_MODEL_PATH=${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_HF_REPO_ID }}" >> $GITHUB_ENV

      - name: set-derivative-env-vars-3
        run: |
          echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.LOCAL_MODEL_PATH}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}" >> $GITHUB_ENV
          echo "LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF=${{env.LOCAL_MODEL_PATH}}/${{env.F16_MMPROJ_MODEL_FILENAME}}" >> $GITHUB_ENV

      - name: verify-github-env
        run: |
          echo "================== Derivative Environment Variables 1 =================="
          echo "TARGET_HF_REPO_ID='$TARGET_HF_REPO_ID' (${{ env.TARGET_HF_REPO_ID }})"
          echo "BASE_NAME_QUANTIZED='$BASE_NAME_QUANTIZED' (${{ env.BASE_NAME_QUANTIZED }})"
          echo "================== Derivative Environment Variables 2 =================="
          echo "BASE_FNAME_QUANTIZED_GGUF='$BASE_FNAME_QUANTIZED_GGUF' (${{ env.BASE_FNAME_QUANTIZED_GGUF }})"
          echo "LOCAL_MODEL_PATH='$LOCAL_MODEL_PATH' (${{ env.LOCAL_MODEL_PATH }})"
          echo "================== Derivative Environment Variables 3 =================="
          echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"
          echo "LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF='$LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF }})"

      - name: ollama-generate-model-name
        run: |
          model_name=$(python ./scripts/get_partner_model_name.py -m ${{env.BASE_NAME_QUANTIZED}} -p ollama)
          echo "model_name: '$model_name'"
          echo "OLLAMA_MODEL_NAME=$model_name" >> $GITHUB_ENV
          model_prefix="${model_name%%:*}"
          echo "OLLAMA_MODEL_PREFIX=$model_prefix" >> $GITHUB_ENV
          model_resources="${{env.OLLAMA_RESOURCE_DIR}}/${model_prefix}"
          echo "OLLAMA_MODEL_RESOURCES=$model_resources" >> $GITHUB_ENV

      # Primarily, we have a projector model that is needed for "vision" (multi-modal) models
      - name: test-for-projector-model
        run: |
          if [[ "${{env.OLLAMA_MODEL_NAME}}" == *"vision"* ]]; then
            echo "HAS_PROJECTOR_MODEL=true" >> $GITHUB_ENV
          else
            echo "NOT a 'vision' model."
          fi

      - name: list-env-vars
        run: env | sort

      # Verify we have Ollama credentials before starting costly downloads
      - name: decode-ollama-data
        run: |
          mkdir $HOME/.ollama
          python ./scripts/decode_ollama_data.py -v ${{secrets.OLLAMA_TOKEN}} -f $HOME/.ollama/id_ed25519 --debug
          ls -al $HOME/.ollama

      - name: verify-quantized-model-exists
        run: |
          exists=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_HF_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN_IBM_GRANITE}})
          echo "exists: '$exists'"
          if [[ "$exists" == "False" ]]; then
            echo "FAILURE: model file: '${{env.TARGET_HF_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' does not exist."
            exit 2
          else
            echo "SUCCESS: model file: '${{env.TARGET_HF_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' exists."
            echo setting environment variable: QUANTIZED_MODEL_EXISTS='true'...
            echo "QUANTIZED_MODEL_EXISTS=true" >> $GITHUB_ENV
          fi

      - name: hf-download-quantized-gguf
        if: env.QUANTIZED_MODEL_EXISTS == 'true'
        run: |
          echo "Downloading model to: ${{env.LOCAL_FNAME_QUANTIZED_GGUF}}..."
          echo "--------------------"
          python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_HF_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN_IBM_GRANITE}}
          ls -al ${{env.LOCAL_FNAME_QUANTIZED_GGUF}}

      - name: hf-download-quantized-projector-gguf
        if: env.HAS_PROJECTOR_MODEL == 'true'
        run: |
          echo "Downloading projector model to: ${{env.LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF}}..."
          echo "--------------------"
          python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_HF_REPO_ID }} ${{ env.F16_MMPROJ_MODEL_FILENAME }} ${{secrets.secrets.HF_TOKEN_IBM_GRANITE}}
          ls -al ${{env.LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF}}

      - name: verify-downloaded-files
        run: |
          echo downloaded files...
          echo "--------------------"
          find . -name \*.gguf -type f
          echo "--------------------"
          if [ -f ${{env.LOCAL_FNAME_QUANTIZED_GGUF}} ]; then
            echo "File exists"
          else
            echo "File does not exist"
            exit 1
          fi

      - name: customize-readme
        run: |
          sed -i '' 's/\<BASE_MODEL_REPO_ORG\>/${{env.REPO_ORG}}/g' ./resources/README.md
          sed -i '' 's/\<BASE_MODEL_REPO_NAME\>/${{env.REPO_NAME}}/g' ./resources/README.md
          cat ./resources/README.md

      - name: ollama-create-modelfile
        run: |
          python ./scripts/create_ollama_model_file.py \
            -m ${{env.LOCAL_FNAME_QUANTIZED_GGUF}} \
            -mp ${{env.LOCAL_FNAME_QUANTIZED_MMPROJ_GGUF}} \
            -l ${{env.OLLAMA_LICENSE_FILE}} \
            -p ${{env.OLLAMA_MODEL_RESOURCES}} \
            -tf ${{env.OLLAMA_TEMPLATE_FILE}} \
            -sf ${{env.OLLAMA_SYSTEM_FILE}} \
            -pf ${{env.OLLAMA_PARAMS_FILE}} \
            -o ${{env.OLLAMA_MODELFILE}}
          echo "Verifying Modelfile..."
          cat ${{env.OLLAMA_MODELFILE}}

      - name: ollama-server-start
        run: |
          echo -e "OLLAMA_DEBUG: $OLLAMA_DEBUG\n"
          echo -e "OLLAMA_PORT: $OLLAMA_PORT\n"
          echo -e "OLLAMA_HOST: $OLLAMA_HOST\n"
          echo -e "starting ollama on port: $OLLAMA_HOST...\n"
          ./bin/ollama serve &
          echo -e "Sleeping...\n"
          sleep 10
          echo -e "Getting PID...\n"
          lsof -i  | grep ollama
          ls -al $HOME/.ollama

      - name: ollama-client
        run: |
          echo -e "Ollama list...\n"
          ./bin/ollama list

      - name: ollama-create-model
        run: |
          echo -e "Ollama creating model from Modelfile...\n"
          ./bin/ollama create ${{env.BASE_NAME_QUANTIZED}} -f Modelfile
          sleep 2
          echo -e "Ollama list...\n"
          ./bin/ollama list

      - name: ollama-model-cp
        run: |
          echo "env.OLLAMA_MODEL_NAME: '${{env.OLLAMA_MODEL_NAME}}'"
          ./bin/ollama cp "${{env.BASE_NAME_QUANTIZED}}" "${{env.TARGET_OLLAMA_ORG}}/${{env.OLLAMA_MODEL_NAME}}"
          sleep 2
          echo -e "Ollama list...\n"
          ./bin/ollama list

      - name: ollama-push-start
        run: |
          echo "OLLAMA_PUSH_START=$(date +'%Y-%m-%d %H:%M:%S.%3N')" >> $GITHUB_ENV

      - name: ollama-model-push
        run: |
          echo -e "Ollama push ...\n"
          ./bin/ollama push ${{env.TARGET_OLLAMA_ORG}}/${{env.OLLAMA_MODEL_NAME}}

      - name: ollama-push-stop
        run: |
          echo "OLLAMA_PUSH_STOP=$(date +'%Y-%m-%d %H:%M:%S.%3N')" >> $GITHUB_ENV

      - name: ollama-is-model-default-quantization
        continue-on-error: true
        run: |
          result=$(python ./scripts/is_model_quant_default.py ${{ inputs.hf_collection_config }} -m ${{env.REPO_NAME}} -q ${{inputs.quantization}})

          if [[ "$result" == "true" ]]; then
            echo "Default model"
            echo "IS_DEFAULT_QUANT=true" >> $GITHUB_ENV
            model_name=$(python ./scripts/get_partner_model_name.py -m ${{env.BASE_NAME_QUANTIZED}} -p ollama --default-quant)
            echo "model_name (default): '$model_name'"
            echo "OLLAMA_MODEL_NAME=$model_name" >> $GITHUB_ENV
          else
            echo "NOT the default model"
          fi

      - name: ollama-push-model-default-quantization
        run: |
          if [[ "$REPO_NAME" =~ "base" ]]; then
            echo "Environment variable contains 'base'. Exiting."
            exit 0
          fi
          if [[ "$IS_DEFAULT_QUANT" == "true" ]]; then
            echo "IS_DEFAULT_QUANT is true."
            echo "env.OLLAMA_MODEL_NAME: '${{env.OLLAMA_MODEL_NAME}}'"
            ./bin/ollama cp "${{env.BASE_NAME_QUANTIZED}}" "${{env.TARGET_OLLAMA_ORG}}/${{env.OLLAMA_MODEL_NAME}}"
            sleep 2
            echo -e "Ollama list...\n"
            ./bin/ollama list
            echo -e "Ollama push ...\n"
            ./bin/ollama push ${{env.TARGET_OLLAMA_ORG}}/${{env.OLLAMA_MODEL_NAME}}
          else
            echo "IS_DEFAULT_QUANT is false."
          fi
          exit 0

      - name: ollama-server-stop
        run:
          echo -e "OLLAMA_PORT:" "${{env.OLLAMA_PORT}}\n"
          echo -e "Killing server...\n"
          kill -9 $(lsof -i:${{env.OLLAMA_PORT}} -t)
          echo -e "Sleeping...\n"
          sleep 10
          lsof -i
          exit 0
